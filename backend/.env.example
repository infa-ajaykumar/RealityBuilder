# Backend System Configuration
# Copy this file to .env in the 'backend' directory and customize the values as needed.
# Docker Compose will automatically load variables from a '.env' file in the same directory where docker-compose is run.

# --- Global Settings ---
NODE_ENV=development

# --- RabbitMQ Configuration ---
# Used by: craigslist_scraper, advanced_scraper_puppeteer, data_processing
RABBITMQ_DEFAULT_USER=user
RABBITMQ_DEFAULT_PASS=password
RABBITMQ_HOST=rabbitmq_server # Service name in Docker Compose
# Constructed URL for Node.js services (advanced_scraper_puppeteer, data_processing)
RABBITMQ_URL=amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@${RABBITMQ_HOST}
RABBITMQ_QUEUE=property_listings_raw # Used by all services interacting with RabbitMQ

# --- PostgreSQL Configuration ---
# Used by: data_processing, api_service, and postgres_db container itself
POSTGRES_USER=user
POSTGRES_PASSWORD=password
POSTGRES_DB=real_estate_db
POSTGRES_HOST=postgres_db # Service name in Docker Compose
POSTGRES_PORT=5432
# Constructed URL for Node.js services (data_processing, api_service)
POSTGRES_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

# --- Elasticsearch Configuration ---
# Used by: data_processing, api_service, and elasticsearch_db container itself
ELASTICSEARCH_HOST=elasticsearch_db # Service name in Docker Compose
ELASTICSEARCH_PORT=9200
# Full URL for Node.js services (data_processing, api_service)
ELASTICSEARCH_NODE=http://${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}
ELASTICSEARCH_INDEX=properties # Used by data_processing, api_service

# For Elasticsearch Docker container:
ES_JAVA_OPTS="-Xms512m -Xmx512m"
# Note: In docker-compose.yml, boolean values for env vars are often quoted like "false"
XPACK_SECURITY_ENABLED="false" # Set to "false" in docker-compose for the ES container
DISCOVERY_TYPE="single-node" # For ES container

# --- API Service (api_service) ---
API_PORT=3000
# Redis connection for API service
REDIS_HOST=redis_cache_service # Service name in Docker Compose
REDIS_PORT=6379
REDIS_URL=redis://${REDIS_HOST}:${REDIS_PORT}
# Cache TTLs for API service
CACHE_TTL_PROPERTIES=300 # In seconds (e.g., 5 minutes)
CACHE_TTL_METADATA=600   # In seconds (e.g., 10 minutes)
# Rate Limiting for API service
RATE_LIMIT_POINTS=100      # Max requests per IP per duration
RATE_LIMIT_DURATION=60     # Duration in seconds (e.g., 100 requests per 60 seconds)
# RATE_LIMIT_BLOCK_DURATION=3600 # Optional: Block duration in seconds if points consumed

# --- Scrapers Configuration ---
# For advanced_scraper_puppeteer:
PUPPETEER_HEADLESS=true
TARGET_URL_ADVANCED_SCRAPER=http://example-dynamic-site.com/listings # Placeholder, replace with actual target

# For both scrapers (craigslist_scraper and advanced_scraper_puppeteer):
# Comma-separated list of proxy URLs. Examples:
# HTTP_PROXIES=http://proxy1.example.com:8080,http://user:pass@proxy2.example.com:3128
# HTTP_PROXIES=socks5://localhost:9050
HTTP_PROXIES=

# --- Data Processing Service (data_processing) ---
GEOCODER_PROVIDER=openstreetmap # Options: 'openstreetmap', 'google', 'here', etc. (some require API keys)
# GEOCODER_API_KEY=YOUR_GEOCODER_API_KEY_HERE # Needed for providers like Google Maps

# Deduplication thresholds used in data_processing service
DEDUPE_LAT_DEG_THRESHOLD=0.0001 # Approx 11 meters. Used for finding duplicate properties.
DEDUPE_LON_DEG_THRESHOLD=0.0001 # Approx 11 meters at equator.
DEDUPE_TITLE_SIMILARITY_THRESHOLD=0.6 # pg_trgm similarity threshold (0.0 to 1.0)

# --- Scheduler Service (scheduler) ---
# Cron schedules are defined internally in backend/scheduler/crontab
# No external environment variables typically needed for the scheduler service itself,
# as it controls other services via docker-compose.

# --- Puppeteer specific (used in advanced_scraper_puppeteer Dockerfile) ---
# PUPPETEER_SKIP_DOWNLOAD=true # Set in Dockerfile if using system-installed Chromium
# PUPPETEER_EXECUTABLE_PATH= # Set in Dockerfile if using system-installed Chromium
# DEBUG=puppeteer:* # For verbose Puppeteer logs, can be set in docker-compose.yml for advanced_scraper_puppeteer
